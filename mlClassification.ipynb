{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": ".deeplearningclass",
   "display_name": ".deeplearningClass",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#feature selection\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "from info_gain import info_gain\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSVFile(filePath):\n",
    "        with open(filePath, 'r', newline='') as csvfile:\n",
    "            has_header = csv.Sniffer().has_header(csvfile.read(1024))\n",
    "            csvfile.seek(0)  # Rewind.\n",
    "            dialect = csv.Sniffer().sniff(csvfile.readline(), [',',';'])\n",
    "            csvfile.seek(0) \n",
    "            reader = csv.reader(csvfile, dialect)\n",
    "            if(has_header):\n",
    "                next(reader)  # Skip header row.\n",
    "            dataset = pd.DataFrame(reader)\n",
    "        return dataset\n",
    "        #print(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readExcel(filePath):\n",
    "    dataset = pd.read_excel(filePath)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_csv(fname):\n",
    "    if fname.endswith((\".data\", \".csv\")):\n",
    "        return readCSVFile(fname)\n",
    "    elif fname.endswith((\".xlsx\")):\n",
    "        return readExcel(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabels(filePath):\n",
    "    try:\n",
    "        flag = 0\n",
    "        dataset = custom_csv(filePath)\n",
    "        #dataset = pd.read_csv(\"/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/iris/iris.data\")\n",
    "        n = dataset.iloc[:, -1].nunique(dropna=False)\n",
    "        perc = dataset.iloc[:, -1].value_counts(normalize=True)*100\n",
    "        if(len(perc) > len(dataset.iloc[:, 0].value_counts(normalize=True)*100)):  #checking whether 1st column is label\n",
    "            n=dataset.iloc[:, 0].nunique(dropna=False)\n",
    "            flag = 1\n",
    "        if(flag == 1):\n",
    "            return dataset.iloc[:, 0]\n",
    "        else:\n",
    "            return dataset.iloc[:,-1]\n",
    "    except:\n",
    "        print(\"Can not read last column items for\", filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getindependentVariables(filePath):\n",
    "    try:\n",
    "        flag = 0\n",
    "        dataset = custom_csv(filePath)\n",
    "        #dataset = pd.read_csv(\"/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/iris/iris.data\")\n",
    "        n = dataset.iloc[:, -1].nunique(dropna=False)\n",
    "        perc = dataset.iloc[:, -1].value_counts(normalize=True)*100\n",
    "        if(len(perc) > len(dataset.iloc[:, 0].value_counts(normalize=True)*100)):  #checking whether 1st column is label\n",
    "            n=dataset.iloc[:, 0].nunique(dropna=False)\n",
    "            flag = 1\n",
    "        if(flag == 1):\n",
    "            return dataset.iloc[:, 1:]\n",
    "        else:\n",
    "            return dataset.iloc[:,:-1]\n",
    "    except:\n",
    "        print(\"Can not independent variabless for\", filePath)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectFeatures(X, impFeatures, threshold):\n",
    "    X_new = pd.DataFrame()\n",
    "    for idx, value in enumerate(impFeatures):\n",
    "        if(value > threshold):\n",
    "            X_new = pd.concat((X_new, X.iloc[:, idx]), axis=1)\n",
    "    return X_new    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutualInfo(X,Y):\n",
    "    return mutual_info_classif(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gainRatio(X,Y):\n",
    "    info_gain_ratio_values = []\n",
    "    for idx, col in X.iteritems():\n",
    "        info_gain_ratio_values.append(info_gain.info_gain_ratio(col.values, Y.values.tolist()))\n",
    "    return info_gain_ratio_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueClasses(Y):\n",
    "    n = len(Y.unique())\n",
    "    return len(Y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyKNN(filePath, fs):\n",
    "    dataset = custom_csv(filePath)\n",
    "  #  dataset = pd.read_csv(\"/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/numeric datasets/wine/wine.data\")\n",
    "    dataset.head()\n",
    "    X = getindependentVariables(filePath)\n",
    "    Y = getLabels(filePath)\n",
    "    if(fs == \"MI\"):\n",
    "      impF = mutualInfo(X,Y)\n",
    "    elif(fs == \"GR\"):\n",
    "      impF = gainRatio(X,Y)\n",
    "    elif(fs == \"mrmr\"):\n",
    "      impF = mrmr_classif(X, Y, K = 10)\n",
    "    X_new = selectFeatures(X, impF, 0.368)\n",
    "    print(X_new.shape[1])  # add this as a feature ; one of the evaluation criteria\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X_new,Y,test_size=0.25,random_state=0)\n",
    "    neighbors = uniqueClasses(Y)\n",
    "    knn=KNeighborsClassifier(n_neighbors=neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred= knn.predict(X_test)\n",
    "    target_names = (list(range(uniqueClasses(Y))))\n",
    "    for index, item in enumerate(target_names):\n",
    "      target_names[index] = str(item)\n",
    "   # TN = cnf_matrix.values.sum() - (FP + FN + TP) \n",
    "    #tp, fn, fp, tn = metrics.confusion_matrix(y_test, y_pred, labels = getLabels(filePath)).ravel()\n",
    "    report = classification_report(y_test, y_pred,  target_names=target_names, output_dict=True)\n",
    "    return pd.DataFrame(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report should be dataset specific - each output file is for a single dataset\n",
    "\n",
    ",0,1,2,accuracy,macro avg,weighted avg, CA, FS, %features  -> such 4*9 rows in each file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7\n              precision    recall  f1-score    support\n0              0.933333  0.875000  0.903226  16.000000\n1              0.818182  0.857143  0.837209  21.000000\n2              0.375000  0.375000  0.375000   8.000000\naccuracy       0.777778  0.777778  0.777778   0.777778\nmacro avg      0.708838  0.702381  0.705145  45.000000\nweighted avg   0.780337  0.777778  0.778511  45.000000\n9\n              precision    recall  f1-score    support\n0              0.933333  0.875000  0.903226  16.000000\n1              0.818182  0.857143  0.837209  21.000000\n2              0.375000  0.375000  0.375000   8.000000\naccuracy       0.777778  0.777778  0.777778   0.777778\nmacro avg      0.708838  0.702381  0.705145  45.000000\nweighted avg   0.780337  0.777778  0.778511  45.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import *\n",
    "#dataset = readCSVFile(\"/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/numeric datasets/wine/wine.data\")\n",
    "#datasethead()\n",
    "a = (classifyKNN(\"/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/numeric datasets/wine/wine.data\", \"MI\"))\n",
    "print(a.transpose())\n",
    "\n",
    "b = (classifyKNN(\"/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/numeric datasets/wine/wine.data\", \"GR\"))\n",
    "#d = pd.read_csv(\"/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/numeric datasets/wine/wine.data\")\n",
    "#u = uniqueClasses(label)\n",
    "#tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "\n",
    "print(b.transpose())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-214af4ab24f9>, line 12)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-214af4ab24f9>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    perfMetrics =\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "listofFiles={}\n",
    "classificationPerfomance={}\n",
    "i=1\n",
    "j=1\n",
    "classificationAlgo = {'knn', 'c45', 'nb', 'svm'}\n",
    "featureAlgo = {'relief', 'relieff', 'ig', 'gr', 'fcbf', 'mrmr', 'qbb', 'focus', 'sc'}\n",
    "for path, subdirs, files in os.walk(os.getcwd()+'/datasets/numeric datasets'):\n",
    "    for name in files:\n",
    "        if name.endswith((\".data\", \".csv\", \".xlsx\")):\n",
    "            listofFiles[name]=os.path.join(path, name)\n",
    "\n",
    "perfMetrics =            \n",
    "for eachFile in listofFiles:\n",
    "    classificationPerfomance[eachFile] = {}\n",
    "    for eachClassAlgo in classificationAlgo:\n",
    "        j+=1\n",
    "        classificationPerfomance[eachFile][eachClassAlgo] = {}\n",
    "        for eachFSAlgo in featureAlgo:\n",
    "            fs = \"FS\"+str(i)\n",
    "            classificationPerfomance[eachFile][eachClassAlgo][eachFSAlgo] = {}\n",
    "            i+=1\n",
    "            if(eachClassAlgo == \"kNN\"):\n",
    "                perfMetrics = classifyKNN(listofFiles[eachFile], eachFSAlgo)\n",
    "            #classificationPerfomance[eachFile]['acc'] = 1\n",
    "\n",
    "        i=1\n",
    "    j=1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "list(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.4544441  0.27522577 0.06626352 0.21696826 0.17967958 0.42101022\n 0.67034758 0.11196006 0.29338146 0.560239   0.47914357 0.50396243\n 0.54630688]\n(177, 9)\n(177, 13)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[15,  1,  0],\n",
       "       [ 0, 19,  1],\n",
       "       [ 0,  0,  9]])"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "from sklearn.feature_selection import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "\n",
    "dataset = pd.read_csv(\"/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/numeric datasets/wine/wine.data\")\n",
    "Y = dataset.iloc[:, :1]\n",
    "X = dataset.iloc[:, 1:]\n",
    "#Y = getLabels(\"/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/iris/iris.data\")\n",
    "\n",
    "importantFeaures = mutual_info_classif(X, Y)\n",
    "print((importantFeaures))\n",
    "\n",
    "X_new = SelectPercentile(mutual_info_classif, percentile=70).fit_transform(X, Y)\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.25,random_state=0)\n",
    "\n",
    "fit = model.fit(X_train, y_train)\n",
    "\n",
    "print(X_new.shape)\n",
    "print(X.shape)\n",
    "y_red=fit.predict(X_test)\n",
    "# import the metrics class\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_red)\n",
    "cnf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=3,p=2,metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "classifier.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[11  1  0]\n [ 0 10  0]\n [ 0  3  5]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test,y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8530286138981791\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_test,y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifaction_report_csv(report):\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    for line in lines[2:-3]:\n",
    "        row = {}\n",
    "        row_data = line.split('      ')\n",
    "        row['class'] = row_data[0]\n",
    "        row['precision'] = float(row_data[1])\n",
    "        row['recall'] = float(row_data[2])\n",
    "        row['f1_score'] = float(row_data[3])\n",
    "        row['support'] = float(row_data[4])\n",
    "        report_data.append(row)\n",
    "    dataframe = pd.DataFrame.from_dict(report_data)\n",
    "    dataframe.to_csv('classification_report.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==============================\n",
      "KNeighborsClassifier\n",
      "****Results****\n",
      "Accuracy: 74.0741%\n",
      "Log Loss: 4.082993941999978\n",
      "==============================\n",
      "DecisionTreeClassifier\n",
      "****Results****\n",
      "Accuracy: 94.4444%\n",
      "Log Loss: 1.918820910828373\n",
      "==============================\n",
      "RandomForestClassifier\n",
      "****Results****\n",
      "Accuracy: 100.0000%\n",
      "Log Loss: 0.14604216622599353\n",
      "==============================\n",
      "AdaBoostClassifier\n",
      "****Results****\n",
      "Accuracy: 92.5926%\n",
      "Log Loss: 1.1277569461761985\n",
      "==============================\n",
      "GradientBoostingClassifier\n",
      "****Results****\n",
      "Accuracy: 92.5926%\n",
      "Log Loss: 0.4291264713030358\n",
      "==============================\n",
      "GaussianNB\n",
      "****Results****\n",
      "Accuracy: 100.0000%\n",
      "Log Loss: 0.013696443178718095\n",
      "==============================\n",
      "LinearDiscriminantAnalysis\n",
      "****Results****\n",
      "Accuracy: 100.0000%\n",
      "Log Loss: 0.003414563937484053\n"
     ]
    }
   ],
   "source": [
    "url = '/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/numeric datasets/wine/wine.data'\n",
    "\n",
    "\n",
    "data = pd.read_csv(url,low_memory=False)\n",
    "x_1 = getindependentVariables(url)\n",
    "data_dia = getLabels(url)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_1, data_dia, test_size=0.3, random_state=42)\n",
    "classifiers = [ \n",
    "KNeighborsClassifier(3),\n",
    "DecisionTreeClassifier(),\n",
    "RandomForestClassifier(),\n",
    "AdaBoostClassifier(),\n",
    "GradientBoostingClassifier(),\n",
    "GaussianNB(),\n",
    "LinearDiscriminantAnalysis() \n",
    "]\n",
    "\n",
    "# Logging for Visual Comparison\n",
    "log_cols = [\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf.fit(x_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "    print(\"=\" * 30)\n",
    "    print(name)\n",
    "\n",
    "    print('****Results****')\n",
    "    train_predictions = clf.predict(x_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "\n",
    "    train_predictions = clf.predict_proba(x_test)\n",
    "    ll = log_loss(y_test, train_predictions)\n",
    "    print(\"Log Loss: {}\".format(ll))\n",
    "\n",
    "    #log_entry = pd.DataFrame([[name, acc * 100, ll]], columns=log_cols)\n",
    "    #log = log.append(log_entry)\n",
    "    #report = classification_report(y_test, train_predictions)\n",
    "    #print(\"log:\",log)\n",
    "    #print(\"=\" * 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mrmr'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b5f51ac59878>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmrmr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrmr_classif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mrmr'"
     ]
    }
   ],
   "source": [
    "from mrmr import mrmr_classif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/shiralkarprashant/FCBF/blob/master/src/fcbf.py\n",
    "def entropy(vec, base=2):\n",
    "\t\" Returns the empirical entropy H(X) in the input vector.\"\n",
    "\t_, vec = np.unique(vec, return_counts=True)\n",
    "\tprob_vec = np.array(vec/float(sum(vec)))\n",
    "\tif base == 2:\n",
    "\t\tlogfn = np.log2\n",
    "\telif base == 10:\n",
    "\t\tlogfn = np.log10\n",
    "\telse:\n",
    "\t\tlogfn = np.log\n",
    "\treturn prob_vec.dot(-logfn(prob_vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(x, y):\n",
    "\t\"Returns H(X|Y).\"\n",
    "\tuy, uyc = np.unique(y, return_counts=True)\n",
    "\tprob_uyc = uyc/float(sum(uyc))\n",
    "\tcond_entropy_x = np.array([entropy(x[y == v]) for v in uy])\n",
    "\treturn prob_uyc.dot(cond_entropy_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(x, y):\n",
    "\t\" Returns the information gain/mutual information [H(X)-H(X|Y)] between two random vars x & y.\"\n",
    "\treturn entropy(x) - conditional_entropy(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetrical_uncertainty(x, y):\n",
    "\t\" Returns 'symmetrical uncertainty' (SU) - a symmetric mutual information measure.\"\n",
    "\n",
    "\treturn 2.0*mutual_information(x, y)/(entropy(x) + entropy(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFirstElement(d):\n",
    "\t\"\"\"\n",
    "\tReturns tuple corresponding to first 'unconsidered' feature\n",
    "\t\n",
    "\tParameters:\n",
    "\t----------\n",
    "\td : ndarray\n",
    "\t\tA 2-d array with SU, original feature index and flag as columns.\n",
    "\t\n",
    "\tReturns:\n",
    "\t-------\n",
    "\ta, b, c : tuple\n",
    "\t\ta - SU value, b - original feature index, c - index of next 'unconsidered' feature\n",
    "\t\"\"\"\n",
    "\n",
    "\tt = np.where(d[:, 2] > 0)[0]\n",
    "\tif len(t):\n",
    "\t\treturn d[t[0], 0], d[t[0], 1], t[0]\n",
    "\treturn None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_correlation(X, y):\n",
    "\t\"\"\"\n",
    "\tReturns SU values between each feature and class.\n",
    "\t\n",
    "\tParameters:\n",
    "\t-----------\n",
    "\tX : 2-D ndarray\n",
    "\t\tFeature matrix.\n",
    "\ty : ndarray\n",
    "\t\tClass label vector\n",
    "\t\t\n",
    "\tReturns:\n",
    "\t--------\n",
    "\tsu : ndarray\n",
    "\t\tSymmetric Uncertainty (SU) values for each feature.\n",
    "\t\"\"\"\n",
    "\tsu = np.zeros(X.shape[1])\n",
    "\tfor i in np.arange(X.shape[1]):\n",
    "\t\tsu[i] = symmetrical_uncertainty(X.iloc[:, i], y)\n",
    "\n",
    "\treturn su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNextElement(d, idx):\n",
    "\t\"\"\"\n",
    "\tReturns tuple corresponding to the next 'unconsidered' feature.\n",
    "\t\n",
    "\tParameters:\n",
    "\t-----------\n",
    "\td : ndarray\n",
    "\t\tA 2-d array with SU, original feature index and flag as columns.\n",
    "\tidx : int\n",
    "\t\tRepresents original index of a feature whose next element is required.\n",
    "\t\t\n",
    "\tReturns:\n",
    "\t--------\n",
    "\ta, b, c : tuple\n",
    "\t\ta - SU value, b - original feature index, c - index of next 'unconsidered' feature\n",
    "\t\"\"\"\n",
    "\tt = np.where(d[:, 2] > 0)[0]\n",
    "\tt = t[t > idx]\n",
    "\tif len(t):\n",
    "\t\treturn d[t[0], 0], d[t[0], 1], t[0]\n",
    "\treturn None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeElement(d, idx):\n",
    "\t\"\"\"\n",
    "\tReturns data with requested feature removed.\n",
    "\t\n",
    "\tParameters:\n",
    "\t-----------\n",
    "\td : ndarray\n",
    "\t\tA 2-d array with SU, original feature index and flag as columns.\n",
    "\tidx : int\n",
    "\t\tRepresents original index of a feature which needs to be removed.\n",
    "\t\t\n",
    "\tReturns:\n",
    "\t--------\n",
    "\td : ndarray\n",
    "\t\tSame as input, except with specific feature removed.\n",
    "\t\"\"\"\n",
    "\td[idx, 2] = 0\n",
    "\treturn d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcbf(X, y, thresh):\n",
    "\t\"\"\"\n",
    "\tPerform Fast Correlation-Based Filter solution (FCBF).\n",
    "\t\n",
    "\tParameters:\n",
    "\t-----------\n",
    "\tX : 2-D ndarray\n",
    "\t\tFeature matrix\n",
    "\ty : ndarray\n",
    "\t\tClass label vector\n",
    "\tthresh : float\n",
    "\t\tA value in [0,1) used as threshold for selecting 'relevant' features. \n",
    "\t\tA negative value suggest the use of minimum SU[i,c] value as threshold.\n",
    "\t\n",
    "\tReturns:\n",
    "\t--------\n",
    "\tsbest : 2-D ndarray\n",
    "\t\tAn array containing SU[i,c] values and feature index i.\n",
    "\t\"\"\"\n",
    "\tn = X.shape[1]\n",
    "\tslist = np.zeros((n, 3))\n",
    "\tslist[:, -1] = 1\n",
    "\n",
    "\t# identify relevant features\n",
    "\tslist[:, 0] = c_correlation(X, y)  # compute 'C-correlation'\n",
    "\tidx = slist[:, 0].argsort()[::-1]\n",
    "\tslist = slist[idx, ]\n",
    "\tslist[:, 1] = idx\n",
    "\tif thresh < 0:\n",
    "\t\tthresh = np.median(slist[-1, 0])\n",
    "\t\tprint(\"Using minimum SU value as default threshold: {0}\".format(thresh))\n",
    "\telif thresh >= 1 or thresh > max(slist[:, 0]):\n",
    "\t\tprint(\"No relevant features selected for given threshold.\")\n",
    "\t\tprint(\"Please lower the threshold and try again.\")\n",
    "\t\texit()\n",
    "\n",
    "\tslist = slist[slist[:, 0] > thresh, :]  # desc. ordered per SU[i,c]\n",
    "\n",
    "\t# identify redundant features among the relevant ones\n",
    "\tcache = {}\n",
    "\tm = len(slist)\n",
    "\tp_su, p, p_idx = getFirstElement(slist)\n",
    "\tfor i in range(m):\n",
    "\t\tp = int(p)\n",
    "\t\tq_su, q, q_idx = getNextElement(slist, p_idx)\n",
    "\t\tif q:\n",
    "\t\t\twhile q:\n",
    "\t\t\t\tq = int(q)\n",
    "\t\t\t\tif (p, q) in cache:\n",
    "\t\t\t\t\tpq_su = cache[(p, q)]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(p)\n",
    "\t\t\t\t\tprint(X[:,  p])\n",
    "\t\t\t\t\t#pq_su = symmetrical_uncertainty(X[:, p], X[:, q])\n",
    "\t\t\t\t\t#cache[(p, q)] = pq_su\n",
    "\n",
    "\t\t\t\t#if pq_su >= q_su:\n",
    "\t\t\t\t#\tslist = removeElement(slist, q_idx)\n",
    "\t\t\t\t#q_su, q, q_idx = getNextElement(slist, q_idx)\n",
    "\n",
    "\t\t#p_su, p, p_idx = getNextElement(slist, p_idx)\n",
    "\t\t#if not p_idx:\n",
    "\t\t#\tbreak\n",
    "\n",
    "\t#sbest = slist[slist[:, 2] > 0, :2]\n",
    "\treturn NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'attrNames'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-aa2201a74085>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0miris\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_iris\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC45\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrNames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'attrNames'"
     ]
    }
   ],
   "source": [
    "from c45 import C45\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "clf = C45(attrNames=iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.9777777777777777\nNumber of mislabeled points out of a total 45 points : 1\n              0          1         2  accuracy  macro avg  weighted avg\nprecision   1.0   1.000000  0.888889  0.977778   0.962963      0.980247\nrecall      1.0   0.952381  1.000000  0.977778   0.984127      0.977778\nf1-score    1.0   0.975610  0.941176  0.977778   0.972262      0.978160\nsupport    16.0  21.000000  8.000000  0.977778  45.000000     45.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "X = getindependentVariables(url)\n",
    "Y = getLabels(url)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.25,random_state=0)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "target_names = (list(range(uniqueClasses(Y))))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "pd.DataFrame(report).to_csv('sample.csv')\n",
    "print(pd.DataFrame(report))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column_in_csv_2(input_file, output_file, transform_row, tansform_column_names):\n",
    "    \"\"\" Append a column in existing csv using csv.reader / csv.writer classes\"\"\"\n",
    "    # Open the input_file in read mode and output_file in write mode\n",
    "    with open(input_file, 'r') as read_obj, \\\n",
    "            open(output_file, 'w', newline='') as write_obj:\n",
    "        # Create a DictReader object from the input file object\n",
    "        dict_reader = DictReader(read_obj)\n",
    "        # Get a list of column names from the csv\n",
    "        field_names = dict_reader.fieldnames\n",
    "        # Call the callback function to modify column name list\n",
    "        tansform_column_names(field_names)\n",
    "        # Create a DictWriter object from the output file object by passing column / field names\n",
    "        dict_writer = DictWriter(write_obj, field_names)\n",
    "        # Write the column names in output csv file\n",
    "        dict_writer.writeheader()\n",
    "        # Read each row of the input csv file as dictionary\n",
    "        for row in dict_reader:\n",
    "            # Modify the dictionary / row by passing it to the transform function (the callback)\n",
    "            transform_row(row, dict_reader.line_num)\n",
    "            # Write the updated dictionary or row to the output file\n",
    "            dict_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.9777777777777777\n              0          1         2  accuracy  macro avg  weighted avg\nprecision   1.0   1.000000  0.888889  0.977778   0.962963      0.980247\nrecall      1.0   0.952381  1.000000  0.977778   0.984127      0.977778\nf1-score    1.0   0.975610  0.941176  0.977778   0.972262      0.978160\nsupport    16.0  21.000000  8.000000  0.977778  45.000000     45.000000\n  Unnamed: 0     0          1         2  accuracy  macro avg  weighted avg\n0  precision   1.0   1.000000  0.888889  0.977778   0.962963      0.980247\n1     recall   1.0   0.952381  1.000000  0.977778   0.984127      0.977778\n2   f1-score   1.0   0.975610  0.941176  0.977778   0.972262      0.978160\n3    support  16.0  21.000000  8.000000  0.977778  45.000000     45.000000\n"
     ]
    }
   ],
   "source": [
    "#working column append n classification report\n",
    "X = getindependentVariables(url)\n",
    "Y = getLabels(url)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.25,random_state=0)\n",
    "\n",
    "clf = svm.SVC(kernel='linear', random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "report = classification_report(y_test, y_pred,  target_names=target_names, output_dict=True)\n",
    "#pd.DataFrame(report).to_csv('sample1.csv')\n",
    "print(pd.DataFrame(report))\n",
    "list_of_str = ['First', 'Second', 'Third', 'Fourth']\n",
    "tail_len = 4\n",
    "\n",
    "# The two steps in the description\n",
    "n_rows = sum(1 for row in open('sample1.csv', 'r'))\n",
    "df = pd.read_csv('sample1.csv', skiprows=range(1, n_rows - tail_len))\n",
    "df_rest  = pd.read_csv('sample1.csv', skiprows=range(tail_len, n_rows))\n",
    "\n",
    "print(df)\n",
    "clsss = 'KNN'\n",
    "percFeatures = 12\n",
    "FSA = 'qwe'\n",
    "df['class']=clsss\n",
    "df['percF'] = percFeatures\n",
    "df['FS'] = FSA\n",
    "df\n",
    "f = []\n",
    "pd.concat([df_rest, df]).to_csv('sampe2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "math domain error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-0c42ecb82e71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetindependentVariables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mimpF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo_gain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo_gain_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mX_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselectFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.368\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/info_gain/info_gain.py\u001b[0m in \u001b[0;36minfo_gain_ratio\u001b[0;34m(Ex, a, nan)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;31m# Compute information gain ratio as IG/IV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minfo_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mintrinsic_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/info_gain/info_gain.py\u001b[0m in \u001b[0;36mintrinsic_value\u001b[0;34m(Ex, a, nan)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mEx_a_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Ex_a_v_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0msum_v\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEx_a_v\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEx_a_v\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m# Return result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "url = '/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/numeric datasets/wine/wine.data'\n",
    "d = custom_csv(url)\n",
    "\n",
    "X = getindependentVariables(url)\n",
    "Y = getLabels(url)\n",
    "impF = info_gain.info_gain_ratio(X, Y)\n",
    "X_new = selectFeatures(X, impF, 0.368)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_new,Y,test_size=0.25,random_state=0)\n",
    "neighbors = uniqueClasses(Y)\n",
    "knn=KNeighborsClassifier(n_neighbors=neighbors)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred= knn.predict(X_test)\n",
    "target_names = (list(range(uniqueClasses(Y))))\n",
    "for index, item in enumerate(target_names):\n",
    "    target_names[index] = str(item)\n",
    "   # TN = cnf_matrix.values.sum() - (FP + FN + TP) \n",
    "    #tp, fn, fp, tn = metrics.confusion_matrix(y_test, y_pred, labels = getLabels(filePath)).ravel()\n",
    "report = classification_report(y_test, y_pred,  target_names=target_names, output_dict=True)\n",
    "print(type(report))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[6, 11, 12, 7, 13, 4, 8, 9, 1, 5]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "from mrmr import mrmr_classif\n",
    "mrmr_classif(X, Y, K = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting skrebate\n",
      "  Downloading skrebate-0.62.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/d19125691/.local/lib/python3.6/site-packages (from skrebate) (1.18.1)\n",
      "Requirement already satisfied: scipy in /home/d19125691/.local/lib/python3.6/site-packages (from skrebate) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn in /home/d19125691/.local/lib/python3.6/site-packages (from skrebate) (0.22.2.post1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/d19125691/.local/lib/python3.6/site-packages (from scikit-learn->skrebate) (1.0.1)\n",
      "Building wheels for collected packages: skrebate\n",
      "  Building wheel for skrebate (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for skrebate: filename=skrebate-0.62-py3-none-any.whl size=29269 sha256=d081a18c74d7342225ac604af14331b0eb4f8d4b0fbeb8d236ad685811b34a30\n",
      "  Stored in directory: /home/d19125691/.cache/pip/wheels/86/52/36/f89dbee69f4529cd18215e030111c80b743b84205ea75f7df6\n",
      "Successfully built skrebate\n",
      "Installing collected packages: skrebate\n",
      "Successfully installed skrebate-0.62\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "from skrebate import ReliefF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'>\n0.44220989760956225\n"
     ]
    }
   ],
   "source": [
    "from info_gain import info_gain\n",
    "\n",
    "# Example of color to indicate whether something is fruit or vegatable\n",
    "produce = ['apple', 'apple', 'apple', 'strawberry', 'eggplant']\n",
    "fruit   = [ True  ,  True  ,  True  ,  True       ,  False    ]\n",
    "colour  = ['green', 'green', 'red'  , 'red'       , 'purple'  ]\n",
    "\n",
    "print(type(produce))\n",
    "\n",
    "igr = info_gain.info_gain_ratio(fruit, colour)\n",
    "igr1 = info_gain.info_gain_ratio(produce, colour)\n",
    "\n",
    "print(igr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting info_gain\n",
      "  Downloading info_gain-1.0.1-py3-none-any.whl (3.3 kB)\n",
      "Installing collected packages: info-gain\n",
      "Successfully installed info-gain-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}