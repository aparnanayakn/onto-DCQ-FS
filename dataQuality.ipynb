{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": ".deeplearningclass",
   "display_name": ".deeplearningClass",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import openpyxl\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSVFile(filePath):\n",
    "    try:\n",
    "        with open(filePath, 'r', newline='') as csvfile:\n",
    "            has_header = csv.Sniffer().has_header(csvfile.read(1024))\n",
    "            csvfile.seek(0)  # Rewind.\n",
    "            dialect = csv.Sniffer().sniff(csvfile.readline(), [',',';'])\n",
    "            csvfile.seek(0) \n",
    "            reader = csv.reader(csvfile, dialect)\n",
    "            if(has_header):\n",
    "                next(reader)  # Skip header row.\n",
    "            dataset = pd.DataFrame(reader)\n",
    "        return dataset\n",
    "        #print(filePath)\n",
    "    except:\n",
    "        print(\"Could not read CSV file\",filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readExcel(filePath):\n",
    "    dataset = pd.read_excel(filePath)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_csv(fname):\n",
    "    if fname.endswith((\".data\", \".csv\")):\n",
    "        return readCSVFile(fname)\n",
    "    elif fname.endswith((\".xlsx\")):\n",
    "        return readExcel(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabels(filePath):\n",
    "    try:\n",
    "        flag = 0\n",
    "        dataset = custom_csv(filePath)\n",
    "        n = dataset.iloc[:, -1].nunique(dropna=False)\n",
    "        perc = dataset.iloc[:, -1].value_counts(normalize=True)*100\n",
    "        if(len(perc) > len(dataset.iloc[:, 0].value_counts(normalize=True)*100)):  #checking whether 1st column is label\n",
    "            n=dataset.iloc[:, 0].nunique(dropna=False)\n",
    "            flag = 1\n",
    "        if(flag == 1):\n",
    "            return dataset.iloc[:, 0]\n",
    "        else:\n",
    "            return dataset.iloc[:,-1]\n",
    "    except:\n",
    "        print(\"Can not read last column items for\", filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeClassEntropy(filePath):\n",
    "    dataset = custom_csv(filePath)\n",
    "    classLabel = getLabels(filePath)\n",
    "    entropy=0\n",
    "    rows = readRows(filePath)\n",
    "    uc = countUniqueLabels(filePath)\n",
    "    values, counts = np.unique(classLabel, return_counts=True)\n",
    "    for i in range(len(values)):\n",
    "        p = counts[i] / rows\n",
    "        entropy -= p * math.log(p,uc)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readRows(filePath):\n",
    "    try:\n",
    "        dataset = custom_csv(filePath)\n",
    "        n = len(dataset.axes[0])\n",
    "        return n\n",
    "    except:\n",
    "        print(\"Can not read rows for\",filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readColumns(filePath):\n",
    "    try:\n",
    "        dataset = custom_csv(filePath)\n",
    "        n = len(dataset.axes[1])\n",
    "        return n\n",
    "    except:\n",
    "        print(\"Can not read columns for\",filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countUniqueLabels(filePath):\n",
    "    try:\n",
    "        dataset = custom_csv(filePath)\n",
    "        n = dataset.iloc[:, -1].nunique(dropna=False)\n",
    "        perc = dataset.iloc[:, -1].value_counts(normalize=True)*100\n",
    "        if(len(perc) > len(dataset.iloc[:, 0].value_counts(normalize=True)*100)):  #checking whether 1st column is label\n",
    "            n=dataset.iloc[:, 0].nunique(dropna=False)\n",
    "        return n\n",
    "    except:\n",
    "        print(\"Can not read unique items for\", filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCorrelation(filePath):\n",
    "    sp=p=sn=n=0\n",
    "    try:\n",
    "        dataset = custom_csv(filePath)\n",
    "        rows, cols = dataset.shape\n",
    "        corr1 = dataset.corr() #Compute pairwise correlation of columns, excluding NA/null values.\n",
    "\n",
    "        c1 = corr1.unstack()\n",
    "\n",
    "        for i in c1:\n",
    "            if(i==1):\n",
    "                sp+=1\n",
    "            elif(i==-1):\n",
    "                sn+=1\n",
    "            elif(i>0):\n",
    "                p+=1\n",
    "            elif(i<=0):\n",
    "                n+=1\n",
    "        corrDict = {}\n",
    "        sp=sp/(cols*(cols-1))\n",
    "        corrDict['spCorr'] = sp \n",
    "        p=p/(cols*(cols-1))\n",
    "        corrDict['pCorr'] = p\n",
    "        sn=sn/(cols*(cols-1))\n",
    "        corrDict['snCorr'] = sn \n",
    "        n=n/(cols*(cols-1))\n",
    "        corrDict['nCorr'] = n\n",
    "        \n",
    "        return corrDict\n",
    "\n",
    "    except:\n",
    "        print(\"Can not compute correlation for\", filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeClassOverlap(filePath):\n",
    "    m = 0\n",
    "    s = 0 \n",
    "    count = 0\n",
    "    outlier = 0\n",
    "    flag = 0\n",
    "    dataset = custom_csv(filePath)\n",
    "    print(\"Coming here\")\n",
    "    km = KMeans(n_clusters = countUniqueLabels(filePath))\n",
    "    clusters = km.fit_predict(dataset)\n",
    "    # points array will be used to reach the index easy\n",
    "    points = np.empty((0,len(dataset.axes[1])), float)\n",
    "    # distances will be used to calculseetate outliers\n",
    "    distances = np.empty((0,len(dataset.axes[0])), float)   \n",
    "        # getting points and distances\n",
    "    centroids = km.cluster_centers_\n",
    "    for i, center_elem in enumerate(centroids):\n",
    "            # cdist is used to calculate the distance between center and other points\n",
    "        distances = np.append(distances, cdist([center_elem],dataset[clusters == i], 'euclidean')) \n",
    "        points = np.append(points, dataset[clusters == i], axis=0)\n",
    "        \n",
    "    cluster_distance_d = {'cluster':clusters, 'distance':distances}\n",
    "    cluster_distance = pd.DataFrame(cluster_distance_d)\n",
    "\n",
    "    grouped = cluster_distance.groupby(['cluster'], as_index = False)\n",
    "    cluster_statistics = grouped[['distance']].agg([np.mean, np.std]) \n",
    "    \n",
    "    for i in range(len(cluster_distance)):#\n",
    "        for j in range(len(cluster_statistics)):\n",
    "            if(cluster_statistics.index[j]==cluster_distance.iloc[i,0]):\n",
    "                m = cluster_statistics.iloc[j,0]\n",
    "                s =cluster_statistics.iloc[j,1]\n",
    "                flag=1\n",
    "                break\n",
    "            if(flag==1):\n",
    "                if(cluster_distance.iloc[i,1] > (m + 3 * s)):\n",
    "                    outlier+=1\n",
    "                    for k in range(len(cluster_statistics)):\n",
    "                        if(cluster_statistics.index[k]!=cluster_distance.iloc[i,0]):\n",
    "                            dist = cdist([points[i]], [centroids[k]], 'euclidean')\n",
    "                            m1 = cluster_statistics.iloc[k,0]\n",
    "                            s1 = cluster_statistics.iloc[k,1]\n",
    "                            if(dist <= (m1 + 3 * s1)):\n",
    "                                count+=1\n",
    "        \n",
    "    #print(count)\n",
    "    #print(outlier)\n",
    "    return [count/(dataset.shape[0] * dataset.shape[1]), outlier/(dataset.shape[0] * dataset.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeness(filePath):\n",
    "    dataset = custom_csv(filePath)\n",
    "    totalMissing = dataset.isnull().sum().sum()\n",
    "    return (totalMissing/(len(dataset.axes[1]) * len(dataset.axes[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classimbalanceRatio(filePath):\n",
    "    dataset = custom_csv(filePath)\n",
    "    totalClasses = countUniqueLabels(filePath)\n",
    "    perc = dataset.iloc[:, -1].value_counts(normalize=True)*100\n",
    "    if(len(perc) > len(dataset.iloc[:, 0].value_counts(normalize=True)*100)):\n",
    "        perc=dataset.iloc[:, 0].value_counts(normalize=True)*100\n",
    "    count = 0\n",
    "    for idx, item in enumerate(perc):\n",
    "        for j in perc[idx+1:]:\n",
    "            if(abs(item-j) > 30):\n",
    "                count+=abs(item-j)\n",
    "    #print(\"count\",(count))\n",
    "    return (count/(dataset.shape[0]*dataset.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conciseness(filePath):\n",
    "    dataset = custom_csv(filePath)\n",
    "    uniques = dataset.drop_duplicates(keep='first')\n",
    "    return (1 - (uniques.shape[0] * uniques.shape[1]) /(dataset.shape[0] * dataset.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typeCheck(singleCol):\n",
    "    ci=cs=co=cf=cd=cu=0\n",
    "    intType = re.compile(r\"^\\d+$\")\n",
    "    dateType1 = re.compile(r\"[0-9]{4}[-/][0-9]?[0-9]?[-/][0-9]?[0-9]?\")\n",
    "    dateType2 = re.compile(r\"[0-9]?[0-9]?[-/][0-9]?[0-9]?[-/][0-9]{4}\")\n",
    "    stringType = re.compile(\"^[a-zA-Z]+.*\\s*[a-zA-Z]*$\")\n",
    "    floatType = re.compile(r\"[-+]?[0-9]*\\.?[0-9]*\")\n",
    "    uriType = re.compile(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\")\n",
    "\n",
    "    for i in range(len(singleCol)):\n",
    "        if((uriType.match(str(singleCol[i])))):\n",
    "            cu+=1\n",
    "        elif(stringType.match(str(singleCol[i]))):\n",
    "            cs+=1\n",
    "        elif((intType.match(str(singleCol[i]) ))):\n",
    "            ci+=1\n",
    "        elif(dateType1.match(str(singleCol[i]) or dateType2.match(str(singleCol[i])))):\n",
    "            cd+=1\n",
    "        elif(floatType.match(str(singleCol[i]))):\n",
    "            cf+=1\n",
    "        else:\n",
    "            co+=1\n",
    "    daConsidered=['int','str','float','date','uri','other']\n",
    "    #overall=[ci,cs,cf,cd,cu,co]\n",
    "    if(cf > ci):             #column with float values, int gets assigned to ci, coverting it to cf\n",
    "        cf = cf+ci\n",
    "        ci=0\n",
    "    #return overall.index(max(overall))\n",
    "    overall=[ci,cs,cf,cd,cu,co]\n",
    "\n",
    "    return max(overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntaxAccuracy(filePath):\n",
    "    dataset = custom_csv(filePath)\n",
    "    count = 0\n",
    "    invalid = 0\n",
    "    for i in range((dataset.shape[1])):\n",
    "        flag=0\n",
    "        if(dataset.iloc[:, i].dtype == \"object\"):\n",
    "            count = typeCheck(dataset.iloc[:, i])\n",
    "            if(count != dataset.shape[0]):\n",
    "                invalid+=1\n",
    "    return (invalid/dataset.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data_banknote_authentication.csv\n",
      "Can not compute correlation for /home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/numeric datasets/bank note authentication/data_banknote_authentication.csv\n",
      "Coming here\n",
      "wine.data\n",
      "Can not compute correlation for /home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/numeric datasets/wine/wine.data\n",
      "Coming here\n",
      "heart_failure_clinical_records_dataset.csv\n",
      "Can not compute correlation for /home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/numeric datasets/heart failure/heart_failure_clinical_records_dataset.csv\n",
      "Coming here\n",
      "divorce.xlsx\n",
      "Coming here\n"
     ]
    }
   ],
   "source": [
    "listofFiles={}\n",
    "for path, subdirs, files in os.walk(os.getcwd()+'/datasets/numeric datasets'):\n",
    "    for name in files:\n",
    "        if name.endswith((\".data\", \".csv\", \".xlsx\")):\n",
    "            listofFiles[name]=os.path.join(path, name)\n",
    "       # elif name.endswith((\".xls\", \".xlsx\")):\n",
    "        #    listofExcelFiles[name]=os.path.join(path, name)\n",
    "\n",
    "#for key in listofCSVFiles:\n",
    " #   readCSVFile(listofCSVFiles[key])\n",
    "\n",
    "#for key in listofExcelFiles:\n",
    " #   readExcel(listofExcelFiles[key])\n",
    "\n",
    "corrDict = {}\n",
    "dataCharQuality = {}\n",
    "count = []\n",
    "for eachFile in listofFiles:\n",
    "    dataCharQuality[eachFile] = {}\n",
    "    print(eachFile)\n",
    "    dataCharQuality[eachFile]['instances'] = readRows(listofFiles[eachFile])\n",
    "    dataCharQuality[eachFile]['attributes'] = readColumns(listofFiles[eachFile])\n",
    "    dataCharQuality[eachFile]['uniqueClasses'] = countUniqueLabels(listofFiles[eachFile])\n",
    "    dataCharQuality[eachFile]['entropy'] = computeClassEntropy(listofFiles[eachFile])\n",
    "    corrDict = computeCorrelation(listofFiles[eachFile])\n",
    "    #print(dataCharacteristics[eachFile]['entropy'])\n",
    "   # entropyDataframe = groupByColumnEntropy(listofDataFiles[eachFile])\n",
    "    if(corrDict):\n",
    "        dataCharQuality[eachFile].update(corrDict)\n",
    "\n",
    "    count = computeClassOverlap(listofFiles[eachFile])\n",
    "    dataCharQuality[eachFile]['classOverlap'] =  count[0]\n",
    "    dataCharQuality[eachFile]['outlierDetection'] = count[1]\n",
    "    dataCharQuality[eachFile]['completeness'] = completeness(listofFiles[eachFile])\n",
    "    dataCharQuality[eachFile]['imbalanceRatio'] = classimbalanceRatio(listofFiles[eachFile])\n",
    "    dataCharQuality[eachFile]['conciseness'] = conciseness(listofFiles[eachFile])\n",
    "    dataCharQuality[eachFile]['syntaxAccuracy'] = syntaxAccuracy(listofFiles[eachFile])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'data_banknote_authentication.csv': '/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/numeric datasets/bank note authentication/data_banknote_authentication.csv', 'wine.data': '/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/numeric datasets/wine/wine.data', 'heart_failure_clinical_records_dataset.csv': '/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/numeric datasets/heart failure/heart_failure_clinical_records_dataset.csv', 'divorce.xlsx': '/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/numeric datasets/divorce/divorce.xlsx'}\n"
     ]
    }
   ],
   "source": [
    "print((listofFiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataquality.json\",\"w\") as f:\n",
    "    json.dump(dataCharQuality,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not used this\n",
    "def kmeans(X,k,max_iterations=100):\n",
    "    '''\n",
    "    X: multidimensional data\n",
    "    k: number of clusters\n",
    "    max_iterations: number of repetitions before clusters are established\n",
    "    \n",
    "    Steps:\n",
    "    1. Convert data to numpy aray\n",
    "    2. Pick indices of k random point without replacement\n",
    "    3. Find class (P) of each data point using euclidean distance\n",
    "    4. Stop when max_iteration are reached of P matrix doesn't change\n",
    "    \n",
    "    Return:\n",
    "    np.array: containg class of each data point\n",
    "    '''\n",
    "    if isinstance(X, pd.DataFrame):X = X.values\n",
    "    idx = np.random.choice(len(X), k, replace=False)\n",
    "    centroids = X[idx, :]\n",
    "    P = np.argmin(distance.cdist(X, centroids, 'euclidean'),axis=1)\n",
    "    for _ in range(max_iterations):\n",
    "        centroids = np.vstack([X[P==i,:].mean(axis=0) for i in range(k)])\n",
    "        tmp = np.argmin(distance.cdist(X, centroids, 'euclidean'),axis=1)\n",
    "        if np.array_equal(P,tmp):break\n",
    "        P = tmp\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}