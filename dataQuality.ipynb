{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": ".deeplearningclass",
   "display_name": ".deeplearningClass",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import openpyxl\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSVFile(filePath):\n",
    "    try:\n",
    "        with open(filePath, 'r', newline='') as csvfile:\n",
    "            has_header = csv.Sniffer().has_header(csvfile.read(1024))\n",
    "            csvfile.seek(0)  # Rewind.\n",
    "            dialect = csv.Sniffer().sniff(csvfile.readline(), [',',';'])\n",
    "            csvfile.seek(0) \n",
    "            reader = csv.reader(csvfile, dialect)\n",
    "            if(has_header):\n",
    "                next(reader)  # Skip header row.\n",
    "            dataset = pd.DataFrame(reader)\n",
    "        return dataset\n",
    "        #print(filePath)\n",
    "    except:\n",
    "        print(\"Could not read CSV file\",filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readExcel(filePath):\n",
    "    dataset = pd.read_excel(filePath)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_csv(fname):\n",
    "    if fname.endswith((\".data\", \".csv\")):\n",
    "        return readCSVFile(fname)\n",
    "    elif fname.endswith((\".xlsx\")):\n",
    "        return readExcel(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabels(filePath):\n",
    "    try:\n",
    "        flag = 0\n",
    "        dataset = custom_csv(filePath)\n",
    "        n = dataset.iloc[:, -1].nunique(dropna=False)\n",
    "        perc = dataset.iloc[:, -1].value_counts(normalize=True)*100\n",
    "        if(len(perc) > len(dataset.iloc[:, 0].value_counts(normalize=True)*100)):  #checking whether 1st column is label\n",
    "            n=dataset.iloc[:, 0].nunique(dropna=False)\n",
    "            flag = 1\n",
    "        if(flag == 1):\n",
    "            return dataset.iloc[:, 0]\n",
    "        else:\n",
    "            return dataset.iloc[:,-1]\n",
    "    except:\n",
    "        print(\"Can not read last column items for\", filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeClassEntropy(filePath):\n",
    "    dataset = custom_csv(filePath)\n",
    "    classLabel = getLabels(filePath)\n",
    "    entropy=0\n",
    "    rows = readRows(filePath)\n",
    "    uc = countUniqueLabels(filePath)\n",
    "    values, counts = np.unique(classLabel, return_counts=True)\n",
    "    for i in range(len(values)):\n",
    "        p = counts[i] / rows\n",
    "        entropy -= p * math.log(p,uc)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readRows(filePath):\n",
    "    try:\n",
    "        dataset = custom_csv(filePath)\n",
    "        n = len(dataset.axes[0])\n",
    "        return n\n",
    "    except:\n",
    "        print(\"Can not read rows for\",filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readColumns(filePath):\n",
    "    try:\n",
    "        dataset = custom_csv(filePath)\n",
    "        n = len(dataset.axes[1])\n",
    "        return n\n",
    "    except:\n",
    "        print(\"Can not read columns for\",filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countUniqueLabels(filePath):\n",
    "    try:\n",
    "        dataset = custom_csv(filePath)\n",
    "        n = dataset.iloc[:, -1].nunique(dropna=False)\n",
    "        perc = dataset.iloc[:, -1].value_counts(normalize=True)*100\n",
    "        if(len(perc) > len(dataset.iloc[:, 0].value_counts(normalize=True)*100)):  #checking whether 1st column is label\n",
    "            n=dataset.iloc[:, 0].nunique(dropna=False)\n",
    "        return n\n",
    "    except:\n",
    "        print(\"Can not read unique items for\", filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCorrelation(filePath):\n",
    "    sp=p=sn=n=0\n",
    "    try:\n",
    "        dataset = custom_csv(filePath)\n",
    "        rows, cols = dataset.shape\n",
    "        corr1 = dataset.corr() #Compute pairwise correlation of columns, excluding NA/null values.\n",
    "\n",
    "        c1 = corr1.unstack()\n",
    "\n",
    "        for i in c1:\n",
    "            if(i==1):\n",
    "                sp+=1\n",
    "            elif(i==-1):\n",
    "                sn+=1\n",
    "            elif(i>0):\n",
    "                p+=1\n",
    "            elif(i<=0):\n",
    "                n+=1\n",
    "        corrDict = {}\n",
    "        sp=sp/(cols*(cols-1))\n",
    "        corrDict['spCorr'] = sp \n",
    "        p=p/(cols*(cols-1))\n",
    "        corrDict['pCorr'] = p\n",
    "        sn=sn/(cols*(cols-1))\n",
    "        corrDict['snCorr'] = sn \n",
    "        n=n/(cols*(cols-1))\n",
    "        corrDict['nCorr'] = n\n",
    "        \n",
    "        return corrDict\n",
    "\n",
    "    except:\n",
    "        print(\"Can not compute correlation for\", filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeClassOverlap(filePath):\n",
    "    m = 0\n",
    "    s = 0 \n",
    "    count = 0\n",
    "    outlier = 0\n",
    "    flag = 0\n",
    "    dataset = custom_csv(filePath)\n",
    "    km = KMeans(n_clusters = countUniqueLabels(filePath))\n",
    "    clusters = km.fit_predict(dataset)\n",
    "    # points array will be used to reach the index easy\n",
    "    points = np.empty((0,len(dataset.axes[1])), float)\n",
    "    # distances will be used to calculseetate outliers\n",
    "    distances = np.empty((0,len(dataset.axes[0])), float)   \n",
    "        # getting points and distances\n",
    "    centroids = km.cluster_centers_\n",
    "    for i, center_elem in enumerate(centroids):\n",
    "            # cdist is used to calculate the distance between center and other points\n",
    "        distances = np.append(distances, cdist([center_elem],dataset[clusters == i], 'euclidean')) \n",
    "        points = np.append(points, dataset[clusters == i], axis=0)\n",
    "        \n",
    "    cluster_distance_d = {'cluster':clusters, 'distance':distances}\n",
    "    cluster_distance = pd.DataFrame(cluster_distance_d)\n",
    "\n",
    "    grouped = cluster_distance.groupby(['cluster'], as_index = False)\n",
    "    cluster_statistics = grouped[['distance']].agg([np.mean, np.std]) \n",
    "    \n",
    "    for i in range(len(cluster_distance)):#\n",
    "        for j in range(len(cluster_statistics)):\n",
    "            if(cluster_statistics.index[j]==cluster_distance.iloc[i,0]):\n",
    "                m = cluster_statistics.iloc[j,0]\n",
    "                s =cluster_statistics.iloc[j,1]\n",
    "                flag=1\n",
    "                break\n",
    "            if(flag==1):\n",
    "                if(cluster_distance.iloc[i,1] > (m + 3 * s)):\n",
    "                    outlier+=1\n",
    "                    for k in range(len(cluster_statistics)):\n",
    "                        if(cluster_statistics.index[k]!=cluster_distance.iloc[i,0]):\n",
    "                            dist = cdist([points[i]], [centroids[k]], 'euclidean')\n",
    "                            m1 = cluster_statistics.iloc[k,0]\n",
    "                            s1 = cluster_statistics.iloc[k,1]\n",
    "                            if(dist <= (m1 + 3 * s1)):\n",
    "                                count+=1\n",
    "        \n",
    "    #print(count)\n",
    "    #print(outlier)\n",
    "    return [count/(dataset.shape[0] * dataset.shape[1]), outlier/(dataset.shape[0] * dataset.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeness(filePath):\n",
    "    dataset = custom_csv(filePath)\n",
    "    totalMissing = dataset.isnull().sum().sum()\n",
    "    return (totalMissing/(len(dataset.axes[1]) * len(dataset.axes[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classimbalanceRatio(filePath):\n",
    "    dataset = custom_csv(filePath)\n",
    "    totalClasses = countUniqueLabels(filePath)\n",
    "    perc = dataset.iloc[:, -1].value_counts(normalize=True)*100\n",
    "    if(len(perc) > len(dataset.iloc[:, 0].value_counts(normalize=True)*100)):\n",
    "        perc=dataset.iloc[:, 0].value_counts(normalize=True)*100\n",
    "    count = 0\n",
    "    for idx, item in enumerate(perc):\n",
    "        for j in perc[idx+1:]:\n",
    "            if(abs(item-j) > 30):\n",
    "                count+=abs(item-j)\n",
    "    #print(\"count\",(count))\n",
    "    return (count/(dataset.shape[0]*dataset.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conciseness(filePath):\n",
    "    dataset = custom_csv(filePath)\n",
    "    uniques = dataset.drop_duplicates(keep='first')\n",
    "    return (1 - (uniques.shape[0] * uniques.shape[1]) /(dataset.shape[0] * dataset.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typeCheck(singleCol):\n",
    "    ci=cs=co=cf=cd=cu=0\n",
    "    intType = re.compile(r\"^\\d+$\")\n",
    "    dateType1 = re.compile(r\"[0-9]{4}[-/][0-9]?[0-9]?[-/][0-9]?[0-9]?\")\n",
    "    dateType2 = re.compile(r\"[0-9]?[0-9]?[-/][0-9]?[0-9]?[-/][0-9]{4}\")\n",
    "    stringType = re.compile(\"^[a-zA-Z]+.*\\s*[a-zA-Z]*$\")\n",
    "    floatType = re.compile(r\"[-+]?[0-9]*\\.?[0-9]*\")\n",
    "    uriType = re.compile(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\")\n",
    "\n",
    "    for i in range(len(singleCol)):\n",
    "        if((uriType.match(str(singleCol[i])))):\n",
    "            cu+=1\n",
    "        elif(stringType.match(str(singleCol[i]))):\n",
    "            cs+=1\n",
    "        elif((intType.match(str(singleCol[i])))):\n",
    "            ci+=1\n",
    "        elif(dateType1.match(str(singleCol[i]) or dateType2.match(str(singleCol[i])))):\n",
    "            cd+=1\n",
    "        elif(floatType.match(str(singleCol[i]))):\n",
    "            cf+=1\n",
    "        else:\n",
    "            co+=1\n",
    "    daConsidered=['int','str','float','date','uri','other']\n",
    "    #overall=[ci,cs,cf,cd,cu,co]\n",
    "    if(cf > ci):             #column with float values, int gets assigned to ci, coverting it to cf\n",
    "        cf = cf+ci\n",
    "        ci=0\n",
    "    #return overall.index(max(overall))\n",
    "    overall=[ci,cs,cf,cd,cu,co]\n",
    "\n",
    "    return max(overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntaxAccuracy(filePath):\n",
    "    dataset = custom_csv(filePath)\n",
    "    count = 0\n",
    "    invalid = 0\n",
    "    for i in range((dataset.shape[1])):\n",
    "        flag=0\n",
    "        if(dataset.iloc[:, i].dtype == \"object\"):\n",
    "            count = typeCheck(dataset.iloc[:, i])\n",
    "            if(count != dataset.shape[0]):\n",
    "                invalid+=1\n",
    "    return (invalid/dataset.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c097407c4c02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlistofFiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/datasets/numeric datasets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c097407c4c02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlistofFiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/datasets/numeric datasets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.vscode/extensions/ms-python.python-2021.3.573419092-dev/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_line\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_cmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_step_cmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydev_original_step_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0mis_return\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# return event\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m                         \u001b[0mback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.vscode/extensions/ms-python.python-2021.3.573419092-dev/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py\u001b[0m in \u001b[0;36mdo_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;31m# IFDEF CYTHON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.vscode/extensions/ms-python.python-2021.3.573419092-dev/pythonFiles/lib/python/debugpy/_vendored/pydevd/pydevd.py\u001b[0m in \u001b[0;36mdo_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads_suspended_single_notification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify_thread_suspended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1855\u001b[0;31m                 \u001b[0mkeep_suspended\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuspend_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_this_thread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \u001b[0mframes_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.vscode/extensions/ms-python.python-2021.3.573419092-dev/pythonFiles/lib/python/debugpy/_vendored/pydevd/pydevd.py\u001b[0m in \u001b[0;36m_do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   1888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_internal_commands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel_async_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_current_thread_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "listofFiles={}\n",
    "for path, subdirs, files in os.walk(os.getcwd()+'/datasets/numeric datasets'):\n",
    "    for name in files:\n",
    "        if name.endswith((\".data\", \".csv\", \".xlsx\")):\n",
    "            listofFiles[name]=os.path.join(path, name)\n",
    "       # elif name.endswith((\".xls\", \".xlsx\")):\n",
    "        #    listofExcelFiles[name]=os.path.join(path, name)\n",
    "\n",
    "#for key in listofCSVFiles:\n",
    " #   readCSVFile(listofCSVFiles[key])\n",
    "\n",
    "#for key in listofExcelFiles:\n",
    " #   readExcel(listofExcelFiles[key])\n",
    "\n",
    "corrDict = {}\n",
    "dataCharQuality = {}\n",
    "count = []\n",
    "for eachFile in listofFiles:\n",
    "    dataCharQuality[eachFile] = {}\n",
    "    print(eachFile)\n",
    "    dataCharQuality[eachFile]['instances'] = readRows(listofFiles[eachFile])\n",
    "    dataCharQuality[eachFile]['attributes'] = readColumns(listofFiles[eachFile])\n",
    "    dataCharQuality[eachFile]['uniqueClasses'] = countUniqueLabels(listofFiles[eachFile])\n",
    "    dataCharQuality[eachFile]['entropy'] = computeClassEntropy(listofFiles[eachFile])\n",
    "    corrDict = computeCorrelation(listofFiles[eachFile])\n",
    "    #print(dataCharacteristics[eachFile]['entropy'])\n",
    "   # entropyDataframe = groupByColumnEntropy(listofDataFiles[eachFile])\n",
    "    if(corrDict):\n",
    "        dataCharQuality[eachFile].update(corrDict)\n",
    "\n",
    "    count = computeClassOverlap(listofFiles[eachFile])\n",
    "    dataCharQuality[eachFile]['classOverlap'] =  count[0]\n",
    "    dataCharQuality[eachFile]['outlierDetection'] = count[1]\n",
    "    dataCharQuality[eachFile]['completeness'] = completeness(listofFiles[eachFile])\n",
    "    dataCharQuality[eachFile]['imbalanceRatio'] = classimbalanceRatio(listofFiles[eachFile])\n",
    "    dataCharQuality[eachFile]['conciseness'] = conciseness(listofFiles[eachFile])\n",
    "    dataCharQuality[eachFile]['syntaxAccuracy'] = syntaxAccuracy(listofFiles[eachFile])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'data_banknote_authentication.csv': '/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/numeric datasets/bank note authentication/data_banknote_authentication.csv', 'wine.data': '/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/numeric datasets/wine/wine.data', 'heart_failure_clinical_records_dataset.csv': '/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/numeric datasets/heart failure/heart_failure_clinical_records_dataset.csv', 'divorce.xlsx': '/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/numeric datasets/divorce/divorce.xlsx'}\n"
     ]
    }
   ],
   "source": [
    "print((listofFiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataquality.json\",\"w\") as f:\n",
    "    json.dump(dataCharQuality,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not used this\n",
    "def kmeans(X,k,max_iterations=100):\n",
    "    '''\n",
    "    X: multidimensional data\n",
    "    k: number of clusters\n",
    "    max_iterations: number of repetitions before clusters are established\n",
    "    \n",
    "    Steps:\n",
    "    1. Convert data to numpy aray\n",
    "    2. Pick indices of k random point without replacement\n",
    "    3. Find class (P) of each data point using euclidean distance\n",
    "    4. Stop when max_iteration are reached of P matrix doesn't change\n",
    "    \n",
    "    Return:\n",
    "    np.array: containg class of each data point\n",
    "    '''\n",
    "    if isinstance(X, pd.DataFrame):X = X.values\n",
    "    idx = np.random.choice(len(X), k, replace=False)\n",
    "    centroids = X[idx, :]\n",
    "    P = np.argmin(distance.cdist(X, centroids, 'euclidean'),axis=1)\n",
    "    for _ in range(max_iterations):\n",
    "        centroids = np.vstack([X[P==i,:].mean(axis=0) for i in range(k)])\n",
    "        tmp = np.argmin(distance.cdist(X, centroids, 'euclidean'),axis=1)\n",
    "        if np.array_equal(P,tmp):break\n",
    "        P = tmp\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}