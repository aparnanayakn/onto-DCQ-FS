{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": ".deeplearningclass",
   "display_name": ".deeplearningClass",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import openpyxl\n",
    "import re\n",
    "import xlsxwriter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSVFile(filePath):\n",
    "        with open(filePath, 'r', newline='',  encoding='utf-8') as csvfile:\n",
    "            has_header = csv.Sniffer().has_header(csvfile.readline())\n",
    "            csvfile.seek(0)  # Rewind.\n",
    "            dialect = csv.Sniffer().sniff(csvfile.read(), delimiters=';,\\t')\n",
    "            csvfile.seek(0) \n",
    "            reader = csv.reader(csvfile, dialect)\n",
    "            if(has_header):\n",
    "                next(reader)  # Skip header row.\n",
    "            dataset = pd.DataFrame(reader)\n",
    "        return dataset\n",
    "        #print(filePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readExcel(filePath):\n",
    "    dataset = pd.read_excel(filePath)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_csv(fname):\n",
    "    if fname.endswith((\".data\", \".csv\")):\n",
    "        return readCSVFile(fname)\n",
    "    elif fname.endswith((\".xlsx\")):\n",
    "        return readExcel(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabels(dataset):\n",
    "    try:\n",
    "        flag = 0\n",
    "        #dataset = custom_csv(filePath)\n",
    "        n = dataset.iloc[:, -1].nunique(dropna=False)\n",
    "        perc = dataset.iloc[:, -1].value_counts(normalize=True)*100\n",
    "        if(len(perc) > len(dataset.iloc[:, 0].value_counts(normalize=True)*100)):  #checking whether 1st column is label\n",
    "            n=dataset.iloc[:, 0].nunique(dropna=False)\n",
    "            flag = 1\n",
    "        if(flag == 1):\n",
    "            return dataset.iloc[:, 0]\n",
    "        else:\n",
    "            return dataset.iloc[:,-1]\n",
    "    except:\n",
    "        print(\"Can not read last column items for\", filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countUniqueLabels(dataset):\n",
    "    try:\n",
    "       # dataset = custom_csv(filePath)\n",
    "        n = getLabels(dataset)\n",
    "        #perc = dataset.iloc[:, -1].value_counts(normalize=True)*100\n",
    "        #if(len(perc) > len(dataset.iloc[:, 0].value_counts(normalize=True)*100)):  #checking whether 1st column is label\n",
    "           # n=dataset.iloc[:, 0].nunique(dropna=False)\n",
    "        return n.nunique(dropna=False)\n",
    "    except:\n",
    "        print(\"Can not read unique items for\", filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeClassEntropy(dataset):\n",
    "   # dataset = custom_csv(filePath)\n",
    "    classLabel = getLabels(dataset)\n",
    "    entropy=0\n",
    "    rows = readRows(dataset)\n",
    "    uc = countUniqueLabels(dataset)\n",
    "    values, counts = np.unique(classLabel, return_counts=True)\n",
    "    for i in range(len(values)):\n",
    "        p = counts[i] / rows\n",
    "        entropy -= p * math.log(p,uc)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readRows(dataset):\n",
    "    try:\n",
    "       # dataset = custom_csv(filePath)\n",
    "        return len(dataset.axes[0])\n",
    "    except:\n",
    "        print(\"Can not read rows for\",filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readColumns(dataset):\n",
    "    try:\n",
    "       # dataset = custom_csv(filePath)\n",
    "        return len(dataset.axes[1])\n",
    "    except:\n",
    "        print(\"Can not read columns for\",filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCorrelation(dataset):\n",
    "    sp=p=sn=n=0\n",
    "       #dataset = custom_csv(filePath)\n",
    "    rows, cols = dataset.shape\n",
    "    corr1 = dataset.corr() #Compute pairwise correlation of columns, excluding NA/null values.\n",
    "\n",
    "    c1 = corr1.unstack()\n",
    "    print(\"Cor\",corr1)\n",
    "    for i in c1:\n",
    "        if(i==1):\n",
    "            sp+=1\n",
    "        elif(i==-1):\n",
    "            sn+=1\n",
    "        elif(i>0):\n",
    "            p+=1\n",
    "        elif(i<=0):\n",
    "            n+=1\n",
    "    corrDict = {}\n",
    "    sp=sp/(cols*(cols-1))\n",
    "    corrDict['spCorr'] = sp \n",
    "    p=p/(cols*(cols-1))\n",
    "    corrDict['pCorr'] = p\n",
    "    sn=sn/(cols*(cols-1))\n",
    "    corrDict['snCorr'] = sn \n",
    "    n=n/(cols*(cols-1))\n",
    "    corrDict['nCorr'] = n\n",
    "        \n",
    "    return corrDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCorrelation(dataset):\n",
    "    sp=p=sn=n=0\n",
    "       #dataset = custom_csv(filePath)\n",
    "    rows, cols = dataset.shape\n",
    "    corr1 = dataset.corr() #Compute pairwise correlation of columns, excluding NA/null values.\n",
    "\n",
    "    c1 = corr1.unstack()\n",
    "    for i in c1:\n",
    "        if(i==1):\n",
    "            sp+=1\n",
    "        elif(i==-1):\n",
    "            sn+=1\n",
    "        elif(i>0):\n",
    "            p+=1\n",
    "        elif(i<=0):\n",
    "            n+=1\n",
    "    corrDict = {}\n",
    "    sp=sp/(cols*(cols-1))\n",
    "    corrDict['spCorr'] = sp \n",
    "    p=p/(cols*(cols-1))\n",
    "    corrDict['pCorr'] = p\n",
    "    sn=sn/(cols*(cols-1))\n",
    "    corrDict['snCorr'] = sn \n",
    "    n=n/(cols*(cols-1))\n",
    "    corrDict['nCorr'] = n\n",
    "        \n",
    "    return sp,p,sn,n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeClassOverlap(dataset):\n",
    "    m = 0\n",
    "    s = 0 \n",
    "    count = 0\n",
    "    outlier = 0\n",
    "    flag = 0\n",
    "   # dataset = custom_csv(filePath)\n",
    "    km = KMeans(n_clusters = countUniqueLabels(dataset))\n",
    "    clusters = km.fit_predict(dataset)\n",
    "    # points array will be used to reach the index easy\n",
    "    points = np.empty((0,len(dataset.axes[1])), float)\n",
    "    # distances will be used to calculseetate outliers\n",
    "    distances = np.empty((0,len(dataset.axes[0])), float)   \n",
    "        # getting points and distances\n",
    "    centroids = km.cluster_centers_\n",
    "    for i, center_elem in enumerate(centroids):\n",
    "            # cdist is used to calculate the distance between center and other points\n",
    "        distances = np.append(distances, cdist([center_elem],dataset[clusters == i], 'euclidean')) \n",
    "        points = np.append(points, dataset[clusters == i], axis=0)\n",
    "        \n",
    "    cluster_distance_d = {'cluster':clusters, 'distance':distances}\n",
    "    cluster_distance = pd.DataFrame(cluster_distance_d)\n",
    "\n",
    "    grouped = cluster_distance.groupby(['cluster'], as_index = False)\n",
    "    cluster_statistics = grouped[['distance']].agg([np.mean, np.std]) \n",
    "    \n",
    "    for i in range(len(cluster_distance)):#\n",
    "        for j in range(len(cluster_statistics)):\n",
    "            if(cluster_statistics.index[j]==cluster_distance.iloc[i,0]):\n",
    "                m = cluster_statistics.iloc[j,0]\n",
    "                s =cluster_statistics.iloc[j,1]\n",
    "                flag=1\n",
    "                break\n",
    "            if(flag==1):\n",
    "                if(cluster_distance.iloc[i,1] > (m + 3 * s)):\n",
    "                    outlier+=1\n",
    "                    for k in range(len(cluster_statistics)):\n",
    "                        if(cluster_statistics.index[k]!=cluster_distance.iloc[i,0]):\n",
    "                            dist = cdist([points[i]], [centroids[k]], 'euclidean')\n",
    "                            m1 = cluster_statistics.iloc[k,0]\n",
    "                            s1 = cluster_statistics.iloc[k,1]\n",
    "                            if(dist <= (m1 + 3 * s1)):\n",
    "                                count+=1\n",
    "        \n",
    "    #print(count)\n",
    "    #print(outlier)\n",
    "    return [count/(dataset.shape[0] * dataset.shape[1]), outlier/(dataset.shape[0] * dataset.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeness(dataset):\n",
    "    #dataset = custom_csv(filePath)\n",
    "    #dataset1 = dataset.dropna(how = 'all', inplace=False)\n",
    "    totalMissing = dataset.isnull().sum().sum()\n",
    "    return (totalMissing /(len(dataset.axes[1]) * len(dataset.axes[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classimbalanceRatio(dataset):\n",
    "    #dataset = custom_csv(filePath)\n",
    "    totalClasses = countUniqueLabels(dataset)\n",
    "    perc = dataset.iloc[:, -1].value_counts(normalize=True)*100\n",
    "    if(len(perc) > len(dataset.iloc[:, 0].value_counts(normalize=True)*100)):\n",
    "        perc=dataset.iloc[:, 0].value_counts(normalize=True)*100\n",
    "    count = 0\n",
    "    for idx, item in enumerate(perc):\n",
    "        for j in perc[idx+1:]:\n",
    "            if(abs(item-j) > 30):\n",
    "                count+=abs(item-j)\n",
    "    #print(\"count\",(count))\n",
    "    return (count/(dataset.shape[0]*dataset.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conciseness(dataset):\n",
    "    #dataset = custom_csv(filePath)\n",
    "    uniques = dataset.drop_duplicates(keep='first')\n",
    "    return (1 - (uniques.shape[0] * uniques.shape[1]) /(dataset.shape[0] * dataset.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typeCheck(singleCol):\n",
    "    ci=cs=co=cf=cd=cu=0\n",
    "    intType = re.compile(r\"^\\d+$\")\n",
    "    dateType1 = re.compile(r\"[0-9]{4}[-/][0-9]?[0-9]?[-/][0-9]?[0-9]?\")\n",
    "    dateType2 = re.compile(r\"[0-9]?[0-9]?[-/][0-9]?[0-9]?[-/][0-9]{4}\")\n",
    "    stringType = re.compile(\"^[a-zA-Z]+.*\\s*[a-zA-Z]*$\")\n",
    "    floatType = re.compile(r\"[-+]?[0-9]*\\.?[0-9]*\")\n",
    "    uriType = re.compile(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\")\n",
    "\n",
    "    for i in range(len(singleCol)):\n",
    "        if((uriType.match(str(singleCol[i])))):\n",
    "            cu+=1\n",
    "        elif(stringType.match(str(singleCol[i]))):\n",
    "            cs+=1\n",
    "        elif((intType.match(str(singleCol[i])))):\n",
    "            ci+=1\n",
    "        elif(dateType1.match(str(singleCol[i]) or dateType2.match(str(singleCol[i])))):\n",
    "            cd+=1\n",
    "        elif(floatType.match(str(singleCol[i]))):\n",
    "            cf+=1\n",
    "        else:\n",
    "            co+=1\n",
    "    daConsidered=['int','str','float','date','uri','other']\n",
    "    #overall=[ci,cs,cf,cd,cu,co]\n",
    "    if(cf > ci):             #column with float values, int gets assigned to ci, coverting it to cf\n",
    "        cf = cf+ci\n",
    "        ci=0\n",
    "    #return overall.index(max(overall))\n",
    "    overall=[ci,cs,cf,cd,cu,co]\n",
    "\n",
    "    return max(overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(x, y):\n",
    "    \"Returns H(X|Y).\"\n",
    "    uy, uyc = np.unique(y, return_counts=True)\n",
    "    prob_uyc = uyc/float(sum(uyc))\n",
    "    cond_entropy_x = np.array([entropy(x[y == v]) for v in uy])\n",
    "    return prob_uyc.dot(cond_entropy_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(x, y):\n",
    "    \" Returns the information gain/mutual information [H(X)-H(X|Y)] between two random vars x & y.\"\n",
    "    return entropy(x) - conditional_entropy(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snr(dataset):    \n",
    " #   dataset = custom_csv(filePath)\n",
    "    classLabel = getLabels(dataset)\n",
    "    columns = list(dataset)\n",
    "    mi=0\n",
    "    for i in range(len(columns)):\n",
    "        mi+=conditional_entropy(classLabel,dataset.iloc[:,i]) \n",
    "    e=0\n",
    "    for i in columns:\n",
    "        e+=entropy(dataset.iloc[:,i])\n",
    "    return (e/dataset.shape[1] - mi/dataset.shape[1])/(mi/dataset.shape[1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(vec, base=2):\n",
    "    q, vec = np.unique(vec, return_counts=True)\n",
    "    prob_vec = np.array(vec/float(sum(vec)))\n",
    "    if base == 2:\n",
    "        logfn = np.log2\n",
    "    elif base == 10:\n",
    "        logfn = np.log10\n",
    "    else:\n",
    "        logfn = np.log\n",
    "    return prob_vec.dot(-logfn(prob_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enattributes(dataset):\n",
    "    cEntropy = computeClassEntropy(dataset)\n",
    "  #  dataset = custom_csv(filePath)\n",
    "    columns = list(dataset)\n",
    "    mi=0\n",
    "    for i in range(len(columns)-1):\n",
    "        col = list(dataset.iloc[:,(i+1):])\n",
    "        for c in col:\n",
    "            mi+=conditional_entropy(dataset.iloc[:,i],dataset.iloc[:,c])  \n",
    "    return (cEntropy / (mi / dataset.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntaxAccuracy(dataset):\n",
    "    #dataset = custom_csv(filePath)\n",
    "    count = 0\n",
    "    invalid = 0\n",
    "    for i in range((dataset.shape[1])):\n",
    "        flag=0\n",
    "        if(dataset.iloc[:, i].dtype == \"object\"):\n",
    "            count = typeCheck(dataset.iloc[:, i])\n",
    "            if(count != dataset.shape[0]):\n",
    "                invalid+=1\n",
    "    return (invalid/dataset.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertstrtointcategory(df): \n",
    "    le = LabelEncoder()\n",
    "    ass1 = _assumption1categorical(df) \n",
    "    ass2 = _assumption2categorical(df)\n",
    "\n",
    "    #extract only columns that belong to \n",
    "    commonidx = (list(set(ass1) | set(ass2)))\n",
    "\n",
    "    for i in commonidx:\n",
    "        df.iloc[:,i] = le.fit_transform(df.iloc[:,i])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryIndex(df): \n",
    "    le = LabelEncoder()\n",
    "    ass1 = _assumption1categorical(df) \n",
    "    ass2 = _assumption2categorical(df)\n",
    "\n",
    "    #extract only columns that belong to \n",
    "    commonidx = (list(set(ass1) | set(ass2)))\n",
    "   # for i in commonidx:\n",
    "     #   df.iloc[:,i] = le.fit_transform(df.iloc[:,i])\n",
    "\n",
    "    return commonidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _assumption1categorical(df):\n",
    "    likely_cat = []\n",
    "    for idx, var in enumerate(df.columns):\n",
    "        if(1.*df[var].nunique()/df[var].count() < 0.05): #or some other threshold\n",
    "            likely_cat.append(idx)\n",
    "    return likely_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _assumption2categorical(df):\n",
    "    top_n = 10 \n",
    "    likely_cat = []\n",
    "    for idx, var in enumerate(df.columns):\n",
    "        if(1.*df[var].value_counts(normalize=True).head(top_n).sum() > 0.8): #or some other threshold\n",
    "            likely_cat.append(idx)\n",
    "    return likely_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAsymmetry(df):\n",
    "    mean = []\n",
    "    median = []\n",
    "    stdDev = []\n",
    "    symmetrical = moderasymmetrical = strongasymmetric = 0\n",
    "    #for col in df.columns:\n",
    "    #print(df.shape)\n",
    "    #print(df.median())\n",
    "    for col in df:\n",
    "        mean.append(df[col].astype(np.float).mean())\n",
    "        stdDev.append(df[col].astype(np.float).std())\n",
    "        median.append(df[col].astype(np.float).median())\n",
    "\n",
    "    \n",
    "    asym = (3*(np.array(mean) - np.array(median))) / np.array(stdDev)\n",
    "    for i in asym:\n",
    "        if(i<0.15):\n",
    "            symmetrical+=1\n",
    "        elif(i>=0.15 and i<1):\n",
    "            moderasymmetrical+=1\n",
    "        else:\n",
    "            strongasymmetric+=1\n",
    "    symmetrical/=df.shape[1]\n",
    "    moderasymmetrical /=df.shape[1]\n",
    "    strongasymmetric /=df.shape[1]\n",
    "\n",
    "    return [symmetrical, moderasymmetrical, strongasymmetric]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wholesale customers data.csv\n",
      "caesarian.csv\n",
      "bank-full.csv\n",
      "data_banknote_authentication.csv\n",
      "heart_failure_clinical_records_datase.csv\n",
      "wine.data\n",
      "HCV-Egy-Data2.csv\n",
      "LasVegasTripAdvisorReviews-Dataset.csv\n",
      "iris.data\n",
      "glass.data\n"
     ]
    }
   ],
   "source": [
    "listofFiles={}\n",
    "for path, subdirs, files in os.walk(os.getcwd()+'/datasets/working'):\n",
    "    for name in files:\n",
    "        if name.endswith((\".data\", \".csv\", \".xlsx\")):\n",
    "            listofFiles[name]=os.path.join(path, name)\n",
    "\n",
    "corrDict = {}\n",
    "dataCharQuality = []\n",
    "\n",
    "temp = []\n",
    "count = []\n",
    "categoryIdx = []\n",
    "for eachFile in listofFiles:\n",
    "    temp = []\n",
    "    temp.append(eachFile)\n",
    "    print(eachFile)\n",
    "    dataset = custom_csv(listofFiles[eachFile])\n",
    "\n",
    "    temp.append(completeness(dataset))\n",
    "    temp.append(classimbalanceRatio(dataset))\n",
    "    temp.append(conciseness(dataset))\n",
    "    temp.append(syntaxAccuracy(dataset))\n",
    "\n",
    "    dataset = convertstrtointcategory(dataset)\n",
    "\n",
    "    count = computeClassOverlap(dataset)\n",
    "    temp.append(count[0])\n",
    "    temp.append(count[1])\n",
    "    temp.append(readRows(dataset))\n",
    "    temp.append(readColumns(dataset))\n",
    "    temp.append(countUniqueLabels(dataset))\n",
    "    temp.append(computeClassEntropy(dataset))\n",
    "    temp.append(snr(dataset))\n",
    "    temp.append(enattributes(dataset))\n",
    "    categoryIdx = categoryIndex(dataset)\n",
    "    asymetry = computeAsymmetry(dataset.drop(categoryIdx, axis=1))\n",
    "    temp.extend(asymetry)\n",
    "\n",
    "    corrDict = computeCorrelation(dataset)\n",
    "   # entropyDataframe = groupByColumnEntropy(listofDataFiles[eachFile])\n",
    "    if(corrDict):\n",
    "      temp.extend(corrDict)\n",
    "    dataCharQuality.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with xlsxwriter.Workbook('test.xlsx') as workbook:\n",
    "    worksheet = workbook.add_worksheet()\n",
    "\n",
    "    for row_num, data in enumerate(dataCharQuality):\n",
    "        worksheet.write_row(row_num, 0, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the csv file in 'w+' mode\n",
    "\n",
    "#write file using xlsx - pending\n",
    "file = open('qwe.csv', 'w+', newline ='')\n",
    "headerInfo = ['fName','completeness','imbalanceRatio', 'conciseness', 'syntaxAccuracy','classOverlap','outlierDetection','instances','attributes','uniqueClasses', 'entropy','snr','ena','symmetrical','modereateasymmetrical','strongasymmetric',  'strongPositive', 'positive', 'strongNegative', 'negative']\n",
    "\n",
    "# writing the data into the file\n",
    "with file:    \n",
    "    write = csv.writer(file)\n",
    "    write.writerow(headerInfo)\n",
    "    write.writerows(dataCharQuality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wholesale customers data.csv\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'dataC' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-7cc316b2ff10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meachFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistofFiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meachFile\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdataC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mdataCharQuality\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meachFile\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'completeness'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompleteness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mdataCharQuality\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meachFile\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'imbalanceRatio'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassimbalanceRatio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataC' is not defined"
     ]
    }
   ],
   "source": [
    "listofFiles={}\n",
    "for path, subdirs, files in os.walk(os.getcwd()+'/datasets/working'):\n",
    "    for name in files:\n",
    "        if name.endswith((\".data\", \".csv\", \".xlsx\")):\n",
    "            listofFiles[name]=os.path.join(path, name)\n",
    "       # elif name.endswith((\".xls\", \".xlsx\")):\n",
    "        #    listofExcelFiles[name]=os.path.join(path, name)\n",
    "\n",
    "#for key in listofCSVFiles:\n",
    " #   readCSVFile(listofCSVFiles[key])\n",
    "\n",
    "#for key in listofExcelFiles:\n",
    " #   readExcel(listofExcelFiles[key])\n",
    "\n",
    "corrDict = {}\n",
    "dataCharQuality = {}\n",
    "count = []\n",
    "categoryIdx = []\n",
    "for eachFile in listofFiles:\n",
    "    dataCharQuality['fName'] = {eachFile}\n",
    "    print(eachFile)\n",
    "    dataset = custom_csv(listofFiles[eachFile])\n",
    "\n",
    "\n",
    "    dataCharQuality[eachFile]['completeness'] = completeness(dataset)\n",
    "    dataCharQuality[eachFile]['imbalanceRatio'] = classimbalanceRatio(dataset)\n",
    "    dataCharQuality[eachFile]['conciseness'] = conciseness(dataset)\n",
    "    dataCharQuality[eachFile]['syntaxAccuracy'] = syntaxAccuracy(dataset)\n",
    "\n",
    "    dataset = convertstrtointcategory(dataset)\n",
    "\n",
    "    count = computeClassOverlap(dataset)\n",
    "    dataCharQuality[eachFile]['classOverlap'] =  count[0]\n",
    "    dataCharQuality[eachFile]['outlierDetection'] = count[1]\n",
    "    dataCharQuality[eachFile]['instances'] = readRows(dataset)\n",
    "    dataCharQuality[eachFile]['attributes'] = readColumns(dataset)\n",
    "    dataCharQuality[eachFile]['uniqueClasses'] = countUniqueLabels(dataset)\n",
    "    dataCharQuality[eachFile]['entropy'] = computeClassEntropy(dataset)\n",
    "    dataCharQuality[eachFile]['snr'] = snr(dataset)\n",
    "    dataCharQuality[eachFile]['ena'] = enattributes(dataset)\n",
    "    categoryIdx = categoryIndex(dataset)\n",
    "    asymetry = computeAsymmetry(dataset.drop(categoryIdx, axis=1))\n",
    "    dataCharQuality[eachFile]['symmetrical'] = asymetry[0]\n",
    "    dataCharQuality[eachFile]['moderasymmetrical'] = asymetry[1]\n",
    "    dataCharQuality[eachFile]['strongasymmetric'] = asymetry[2]\n",
    "\n",
    "    corrDict = computeCorrelation(dataset)\n",
    "    #print(dataCharacteristics[eachFile]['entropy'])\n",
    "   # entropyDataframe = groupByColumnEntropy(listofDataFiles[eachFile])\n",
    "    if(corrDict):\n",
    "        dataCharQuality[eachFile].update(corrDict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataquality.json\",\"w\") as f:\n",
    "    json.dump(dataCharQuality,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not used this\n",
    "def kmeans(X,k,max_iterations=100):\n",
    "    '''\n",
    "    X: multidimensional data\n",
    "    k: number of clusters\n",
    "    max_iterations: number of repetitions before clusters are established\n",
    "    \n",
    "    Steps:\n",
    "    1. Convert data to numpy aray\n",
    "    2. Pick indices of k random point without replacement\n",
    "    3. Find class (P) of each data point using euclidean distance\n",
    "    4. Stop when max_iteration are reached of P matrix doesn't change\n",
    "    \n",
    "    Return:\n",
    "    np.array: containg class of each data point\n",
    "    '''\n",
    "    if isinstance(X, pd.DataFrame):X = X.values\n",
    "    idx = np.random.choice(len(X), k, replace=False)\n",
    "    centroids = X[idx, :]\n",
    "    P = np.argmin(distance.cdist(X, centroids, 'euclidean'),axis=1)\n",
    "    for _ in range(max_iterations):\n",
    "        centroids = np.vstack([X[P==i,:].mean(axis=0) for i in range(k)])\n",
    "        tmp = np.argmin(distance.cdist(X, centroids, 'euclidean'),axis=1)\n",
    "        if np.array_equal(P,tmp):break\n",
    "        P = tmp\n",
    "    return P"
   ]
  }
 ]
}