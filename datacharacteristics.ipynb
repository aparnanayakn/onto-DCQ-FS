{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": ".deeplearningclass",
   "display_name": ".deeplearningClass",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDataFile(filePath):\n",
    "    try:\n",
    "        dataset = pd.read_csv(filePath)\n",
    "        print(filePath)\n",
    "    except:\n",
    "        print(\"Could not read file\",filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/numeric datasets/wine/wine.data\n/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/Lung cancer/lung-cancer.data\n/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/adult/adult.data\n/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/iris/iris.data\n/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/glass/glass.data\n/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/Pittsburgh Bridges/bridges.data\n/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/trains/trains-transformed.data\n/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/arrhythmia/arrhythmia.data\n/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/dermatology/dermatology.data\n/home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/ballons/yellow-small+adult-stretch.data\n"
     ]
    }
   ],
   "source": [
    "listofDataFiles={}\n",
    "for path, subdirs, files in os.walk(os.getcwd()+'/datasets'):\n",
    "    for name in files:\n",
    "        if name.endswith('.data'):\n",
    "            listofDataFiles[name]=os.path.join(path, name)\n",
    "\n",
    "for key in listofDataFiles:\n",
    "    readDataFile(listofDataFiles[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readRows(filePath):\n",
    "    try:\n",
    "        dataset = pd.read_csv(filePath)\n",
    "        n = len(dataset.axes[0])\n",
    "        return n\n",
    "    except:\n",
    "        print(\"Can not read rows for\",filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readColumns(filePath):\n",
    "    try:\n",
    "        dataset = pd.read_csv(filePath)\n",
    "        n = len(dataset.axes[1])\n",
    "        return n\n",
    "    except:\n",
    "        print(\"Can not read columns for\",filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countUniqueLabels(filePath):\n",
    "    try:\n",
    "        dataset = pd.read_csv(filePath)\n",
    "        n = dataset.iloc[:, -1].nunique(dropna=False)\n",
    "        return n\n",
    "    except:\n",
    "        print(\"Can not read unique items for\", filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCorrelation(filePath):\n",
    "    sp=p=sn=n=0\n",
    "    try:\n",
    "        dataset = pd.read_csv(filePath)\n",
    "        rows, cols = dataset.shape\n",
    "        corr1 = dataset.corr()\n",
    "\n",
    "        c1 = corr1.abs().unstack()\n",
    "\n",
    "        for i in c1:\n",
    "            if(i==1):\n",
    "                sp+=1\n",
    "            elif(i==-1):\n",
    "                sn+=1\n",
    "            elif(i>0):\n",
    "                p+=1\n",
    "            elif(i<=0):\n",
    "                n+=1\n",
    "        corrDict = {}\n",
    "        sp=sp/(cols*(cols-1))\n",
    "        corrDict['spCorr'] = sp \n",
    "        p=p/(cols*(cols-1))\n",
    "        corrDict['pCorr'] = p\n",
    "        sn=sn/(cols*(cols-1))\n",
    "        corrDict['snCorr'] = sn \n",
    "        n=n/(cols*(cols-1))\n",
    "        corrDict['nCorr'] = n\n",
    "        \n",
    "        return corrDict\n",
    "\n",
    "    except:\n",
    "        print(\"Can not compute correlation for\", filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snratio(entropy,ami):\n",
    "    return ((entropy-ami)/ami)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-8c102c07ebc4>, line 8)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-8c102c07ebc4>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    ig = ig +\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def computeIG(filePath):\n",
    "    dataset = pd.read_csv(filePath)\n",
    "    classLabel = dataset.iloc[:,-1]\n",
    "    dummyData = pd.get_dummies(dataset[:,:-1])\n",
    "    entropy = computeClassEntropy(filePath)\n",
    "    ig = 0\n",
    "\n",
    "    ig = ig + \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeConditionalEntropy(df,classLabel):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeClassEntropy(filePath):\n",
    "    dataset = pd.read_csv(filePath)\n",
    "    classLabel = dataset.iloc[:,-1]\n",
    "    entropy=0\n",
    "    rows = readRows(filePath)\n",
    "    uc = countUniqueLabels(filePath)\n",
    "    values, counts = np.unique(classLabel, return_counts=True)\n",
    "    for i in range(len(values)):\n",
    "        p = counts[i] / rows\n",
    "        entropy -= p * math.log(p,uc)\n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(values):\n",
    "    entropy = - sum( values * np.log(values))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AditionalCode\n",
    "def groupByColumnEntropy(filePath):\n",
    "    ent = []\n",
    "    dataset = pd.read_csv(filePath)\n",
    "    dataset = dataset.replace('?','0')\n",
    "    dataset = pd.get_dummies(dataset)\n",
    "   # dataset[is.na(dataset)] <- 0\n",
    "    classLabel = dataset.iloc[:,-1].values\n",
    "    imdValues = []\n",
    "    df1 = pd.DataFrame(classLabel)\n",
    "    data = dataset.iloc[:,:-1]\n",
    "    for column in data:\n",
    "        imdValues = []\n",
    "        gSum = dataset.groupby(dataset.iloc[:,-1])[column].transform('sum')\n",
    "       # print(type(dataset[column][5]))\n",
    "        values = dataset[column].astype(float)/gSum.astype(float)\n",
    "        imdValues.append(values)\n",
    "        ent=entropy(imdValues)\n",
    "        df1 = pd.concat([df1,pd.DataFrame(ent)], axis=1)\n",
    "        #print(len(ent)==len(classLabel))\n",
    "        #df1 = pd.concat([df1, ent.to_frame()], axis =1, ignore_index=True)\n",
    "    temp = []\n",
    "    df2 = pd.DataFrame()\n",
    "\n",
    "    df2 = df1.groupby(classLabel,as_index=False,sort=False).sum().reset_index()\n",
    "\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "wine.data\n",
      "lung-cancer.data\n",
      "adult.data\n",
      "iris.data\n",
      "glass.data\n",
      "bridges.data\n",
      "Can not compute correlation for /home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/Pittsburgh Bridges/bridges.data\n",
      "trains-transformed.data\n",
      "Can not compute correlation for /home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/trains/trains-transformed.data\n",
      "arrhythmia.data\n",
      "dermatology.data\n",
      "yellow-small+adult-stretch.data\n",
      "Can not compute correlation for /home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS/datasets/ballons/yellow-small+adult-stretch.data\n"
     ]
    }
   ],
   "source": [
    "corrDict = {}\n",
    "dataCharacteristics = {}\n",
    "for eachFile in listofDataFiles:\n",
    "    print(eachFile)\n",
    "    dataCharacteristics[eachFile] = {}\n",
    "    dataCharacteristics[eachFile]['instances'] = readRows(listofDataFiles[eachFile])\n",
    "    dataCharacteristics[eachFile]['attributes'] = readColumns(listofDataFiles[eachFile])\n",
    "    dataCharacteristics[eachFile]['uniqueClasses'] = countUniqueLabels(listofDataFiles[eachFile])\n",
    "    dataCharacteristics[eachFile]['entropy'] = computeClassEntropy(listofDataFiles[eachFile])\n",
    "    corrDict = computeCorrelation(listofDataFiles[eachFile])\n",
    "    #print(dataCharacteristics[eachFile]['entropy'])\n",
    "    entropyDataframe = groupByColumnEntropy(listofDataFiles[eachFile])\n",
    "    if(corrDict):\n",
    "        dataCharacteristics[eachFile].update(corrDict)\n",
    "\n",
    "    #print(corrDict)\n",
    "\n",
    "\n",
    "#for key in dataCharacteristics:\n",
    "  #  print(dataCharacteristics[key]['uniqueClasses'])\n",
    "    #print(dataCharacteristics[key]['attributes'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.json\",\"w\") as f:\n",
    "    json.dump(dataCharacteristics,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     0\n0  tom\n1  tom\n2  tom\n3  tom\n4  eva\n5  eva\n6  eva\n     1    2\n0  700  300\n1  300  700\n2  100  200\n3  200  100\n4  700  300\n5  300  250\n6  250  700\n     0         0         0\n0  tom  0.333329  0.338385\n1  tom  0.338385  0.333329\n2  tom  0.197304  0.287970\n3  tom  0.287970  0.197304\n4  eva  0.324698  0.342508\n5  eva  0.342508  0.321888\n6  eva  0.321888  0.324698\n   index         0         0\n0      0  1.156988  1.156988\n1      1  0.989094  0.989094\n"
     ]
    }
   ],
   "source": [
    "# Online Python compiler (interpreter) to run Python online.\n",
    "# Write Python 3 code in this online editor and run it.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = [['tom',700,300],\n",
    "        ['tom',300,700],\n",
    "        ['tom',100,200],\n",
    "        ['tom',200,100],\n",
    "        ['eva',700,300],\n",
    "        ['eva',300,250],\n",
    "        ['eva',250,700]]\n",
    "s = pd.DataFrame( data)\n",
    "\n",
    "#dummy_data = dummy_data.rename(columns={c:c.replace('0', '') for c in dummy_data.columns})\n",
    "#data = s.iloc[:,1:].values\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "count = 1\n",
    "classLabel = s.iloc[:,:1].values.flatten()\n",
    "df = pd.DataFrame(np.array([classLabel]).T)\n",
    "print(df)\n",
    "\n",
    "data = s.iloc[:,1:]\n",
    "\n",
    "print(data)\n",
    "for column in data:\n",
    "    imdValues = []\n",
    "    gSum = s.groupby(classLabel)[column].transform('sum')\n",
    "    #print(gSum)\n",
    "    val = s[column]/gSum\n",
    "    imdValues.append(val)\n",
    "    ent=entropy(imdValues)\n",
    "    df = pd.concat([df,pd.DataFrame(ent)], axis=1)\n",
    "    count+=1\n",
    "    \n",
    "df2 = df.groupby(classLabel,as_index=False,sort=False).sum().reset_index()\n",
    "\n",
    "return df2\n",
    "\n",
    "#print(imdValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Grouper for '<class 'pandas.core.frame.DataFrame'>' not 1-dimensional",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0c1b73dffd8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#print(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mgSum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgSum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed)\u001b[0m\n\u001b[1;32m   5808\u001b[0m             \u001b[0mgroup_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5809\u001b[0m             \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5810\u001b[0;31m             \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5811\u001b[0m         )\n\u001b[1;32m   5812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                 \u001b[0mmutated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmutated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m             )\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, index, grouper, obj, name, level, sort, observed, in_axis)\u001b[0m\n\u001b[1;32m    343\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ndim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Grouper for '{t}' not 1-dimensional\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 if not (\n",
      "\u001b[0;31mValueError\u001b[0m: Grouper for '<class 'pandas.core.frame.DataFrame'>' not 1-dimensional"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "data = [['tom',700],\n",
    "        ['tom',300],\n",
    "        ['tom',100],\n",
    "        ['tom',200],\n",
    "        ['eva',700],\n",
    "        ['eva',300],\n",
    "        ['eva',250]]\n",
    "s = pd.DataFrame( data )\n",
    "\n",
    "#dummy_data = dummy_data.rename(columns={c:c.replace('0', '') for c in dummy_data.columns})\n",
    "data = s.iloc[:,1:]\n",
    "#print(data)\n",
    "\n",
    "gSum = s.groupby(s.iloc[:,:1])[data].transform('sum')\n",
    "\n",
    "print(gSum)\n",
    "\n",
    "for column in data:\n",
    "    imdValues = []\n",
    "    gSum = data.groupby(s.iloc[:,:1])[column].transform('sum')\n",
    "    print(gSum)\n",
    "    values = s[column]/gSum\n",
    "    imdValues.append(values)\n",
    "\n",
    "\n",
    "df2 = df1.groupby(classLabel,as_index=False,sort=False)['Entropy'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(9, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-47-621762ea50b2>, line 4)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-47-621762ea50b2>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    (float)a\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,'#']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = [['tom',700,300],\n",
    "        ['tom',300,700],\n",
    "        ['tom',100,200],\n",
    "        ['tom',200,100],\n",
    "        ['eva',700,300],\n",
    "        ['eva',300,250],\n",
    "        ['eva',250,700]]\n",
    "s = pd.DataFrame( data)\n",
    "\n",
    "#dummy_data = dummy_data.rename(columns={c:c.replace('0', '') for c in dummy_data.columns})\n",
    "data = s.iloc[:,1:]\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "count = 1\n",
    "classLabel = s.iloc[:,:1].values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['tom' 'tom' 'tom' 'tom' 'eva' 'eva' 'eva']\n"
     ]
    }
   ],
   "source": [
    "print(classLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}